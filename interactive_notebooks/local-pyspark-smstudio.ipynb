{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Local PySpark on SageMaker Studio\n",
                "\n",
                "This notebook shows how to run local PySpark code within a SageMaker Studio notebook. For this example we use the **Data Science - Python3** image and kernel, but this methodology should work for any kernel within SM Studio, including BYO custom images.",
            ],
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup\n",
                "There are two things that must be done to enable local PySpark within SageMaker Studio.\n",
                "1. Make sure there is an available Java installation. The easiest way to install JDK and set the proper paths is to utilize conda\n",
                "2. We need to append the local container's hostname into `/etc/hosts` in order for Spark to properly communicate",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup - Run only once per Kernel App\n",
                "%conda install openjdk -y\n",
                "!grep `hostname` /etc/hosts >/dev/null || echo 127.0.0.1 `hostname` >> /etc/hosts",
            ],
        },
        {"cell_type": "markdown", "metadata": {}, "source": ["## Install PySpark"]},
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ["%pip install pyspark==3.2.0"],
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Utilize S3 Data within local PySpark\n",
                "* By specifying the `hadoop-aws` jar in our Spark config we're able to access S3 datasets using the s3a file prefix. \n",
                "* Since we've already authenticated ourself to SageMaker Studio , we can use our assumed SageMaker ExecutionRole for any S3 reads/writes by setting the credential provider as `ContainerCredentialsProvider`",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {"name": "stdout", "output_type": "stream", "text": ["3.2.0\n"]}
            ],
            "source": [
                "# Import pyspark and build Spark session\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "spark = (\n",
                '    SparkSession.builder.appName("PySparkApp")\n',
                '    .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.2.2")\n',
                "    .config(\n",
                '        "fs.s3a.aws.credentials.provider",\n',
                '        "com.amazonaws.auth.ContainerCredentialsProvider",\n',
                "    )\n",
                "    .getOrCreate()\n",
                ")\n",
                "\n",
                "print(spark.version)",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
                        "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
                        "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
                        "|           HV0003|              B02867|2019-02-01 00:05:18|2019-02-01 00:14:57|         245|         251|   null|\n",
                        "|           HV0003|              B02879|2019-02-01 00:41:29|2019-02-01 00:49:39|         216|         197|   null|\n",
                        "|           HV0005|              B02510|2019-02-01 00:51:34|2019-02-01 01:28:29|         261|         234|   null|\n",
                        "|           HV0005|              B02510|2019-02-01 00:03:51|2019-02-01 00:07:16|          87|          87|   null|\n",
                        "|           HV0005|              B02510|2019-02-01 00:09:44|2019-02-01 00:39:56|          87|         198|   null|\n",
                        "|           HV0005|              B02510|2019-02-01 00:59:55|2019-02-01 01:06:28|         198|         198|      1|\n",
                        "|           HV0005|              B02510|2019-02-01 00:12:06|2019-02-01 00:42:13|         161|         148|   null|\n",
                        "|           HV0005|              B02510|2019-02-01 00:45:35|2019-02-01 01:14:56|         148|          21|   null|\n",
                        "|           HV0003|              B02867|2019-02-01 00:10:48|2019-02-01 00:20:23|         226|         260|   null|\n",
                        "|           HV0003|              B02867|2019-02-01 00:32:32|2019-02-01 00:40:25|           7|         223|   null|\n",
                        "|           HV0003|              B02867|2019-02-01 00:59:54|2019-02-01 01:09:31|         129|          70|   null|\n",
                        "|           HV0003|              B02764|2019-02-01 00:01:11|2019-02-01 00:21:35|         263|         229|   null|\n",
                        "|           HV0003|              B02764|2019-02-01 00:36:22|2019-02-01 00:55:30|         162|         129|   null|\n",
                        "|           HV0002|              B02914|2019-02-01 00:10:09|2019-02-01 00:31:04|         161|          33|   null|\n",
                        "|           HV0003|              B02864|2019-02-01 00:57:50|2019-02-01 01:05:08|         258|         197|   null|\n",
                        "|           HV0003|              B02875|2019-02-01 00:05:24|2019-02-01 00:17:13|         255|          17|   null|\n",
                        "|           HV0003|              B02875|2019-02-01 00:27:38|2019-02-01 00:32:36|         255|         112|   null|\n",
                        "|           HV0003|              B02875|2019-02-01 00:52:50|2019-02-01 00:56:02|         234|         137|   null|\n",
                        "|           HV0003|              B02682|2019-02-01 00:30:24|2019-02-01 00:59:08|         163|         256|   null|\n",
                        "|           HV0003|              B02682|2019-02-01 00:35:06|2019-02-01 00:44:27|         161|         262|   null|\n",
                        "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
                        "only showing top 20 rows\n",
                        "\n",
                    ],
                }
            ],
            "source": [
                "csv_df = spark.read.csv(\n",
                '    "s3a://nyc-tlc/trip data/fhvhv_tripdata_2019-02.csv", header=True\n',
                ")\n",
                "csv_df.show()",
            ],
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [],
        },
    ],
    "metadata": {
        "instance_type": "ml.t3.medium",
        "kernelspec": {
            "display_name": "Python 3 (Data Science)",
            "language": "python",
            "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0",
        },
        "language_info": {
            "codemirror_mode": {"name": "ipython", "version": 3},
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10",
        },
    },
    "nbformat": 4,
    "nbformat_minor": 4,
}
